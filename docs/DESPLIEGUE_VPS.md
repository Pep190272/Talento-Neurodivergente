# üöÄ Despliegue VPS - Ollama + Llama 3.2 3B para Diversia

**Fecha**: 24 de enero de 2026 (actualizado 25 Feb 2026)
**Servidor**: VPS Hostinger (Par√≠s, Francia)
**IP**: 77.83.232.203
**Tecnolog√≠as**: Dokploy, Docker, Ollama, Llama 3.2 3B
**Decision**: Self-hosted definitivo por GDPR Art. 9 (datos neurodivergentes)

---

## üìä Especificaciones del VPS

| Caracter√≠stica | Valor |
|----------------|-------|
| **Proveedor** | Hostinger |
| **CPU** | 2 cores |
| **RAM** | 8 GB |
| **Disco** | 100 GB SSD |
| **OS** | Ubuntu 24.04 LTS |
| **Panel** | Dokploy (pre-instalado) |
| **Ubicaci√≥n** | Par√≠s, Francia üá´üá∑ |

---

## ‚úÖ Proceso de Despliegue (Paso a Paso)

### **PASO 1: Acceso a Dokploy**

1. **Abrir panel de Hostinger**:
   - URL: `https://hpanel.hostinger.com/vps/1929568/overview`

2. **Navegar a "OS & Panel"** en el men√∫ lateral

3. **Hacer clic en "Manage panel"** (arriba a la derecha)
   - Esto abre Dokploy en nueva pesta√±a
   - URL t√≠pica: `http://77.83.232.203:3000`

4. **Iniciar sesi√≥n en Dokploy** con credenciales de Hostinger

---

### **PASO 2: Crear Proyecto en Dokploy**

1. **En Dokploy Dashboard**, ir a **"Projects"** (men√∫ lateral)

2. **Hacer clic en el bot√≥n "+"** para crear nuevo proyecto

3. **Rellenar formulario**:
   - **Project Name**: `Diversia`
   - **Description**: `Impulso del talento neurodivergente`
   - **Application ID**: `diversia-acef0q` (auto-generado)

4. **Hacer clic en "Create"**

5. **Acceder al proyecto** haciendo clic sobre el nombre "Diversia"

---

### **PASO 3: Crear Compose para Ollama**

1. **Dentro del proyecto Diversia**, buscar panel de pesta√±as superior:
   - General
   - Environment
   - Domains
   - **Deployments** ‚Üê Aqu√≠ encontrar√°s opciones

2. **En la secci√≥n "Deploy Settings"**, hacer clic en **"Compose"** (bot√≥n arriba a la derecha)

3. **En "Provider"**, seleccionar **"Raw"** (√∫ltimo bot√≥n)
   - Ignorar GitHub, GitLab, Bitbucket

4. **Pegar el siguiente `docker-compose.yml`**:

```yaml
version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: diversia-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  ollama-data:
    driver: local
```

5. **Hacer clic en "Save"** o "Create"

6. **Hacer clic en "Deploy"**

7. **Esperar 1-2 minutos** mientras descarga la imagen Docker de Ollama

---

### **PASO 4: Acceder al Terminal del Contenedor**

1. **En Dokploy**, dentro del proyecto "Diversia", buscar la lista de servicios

2. **Localizar el servicio** `diversia-ollama` con estado **"running"** (verde)

3. **Hacer clic en el bot√≥n de men√∫** (tres puntitos) del servicio

4. **Seleccionar "Docker Terminal"** o "Open Terminal"

5. **Se abrir√° una terminal web** mostrando:
   ```
   root@2e1c5d75c3f4:/#
   ```

---

### **PASO 5: Descargar Modelo Llama 3.2 3B**

> **Nota**: Originalmente se usaba Gemma 2B. Se actualiz√≥ a Llama 3.2 3B (25 Feb 2026)
> por mejor calidad en seguir instrucciones (IFEval 77.4 vs 61.9).

1. **En la terminal del contenedor**, ejecutar:

```bash
ollama pull llama3.2:3b
```

2. **Esperar la descarga** (~2 GB, tarda 3-5 minutos):
   ```
   pulling manifest
   pulling 4b2ac8... 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB
   success
   ```

3. **Verificar que se descarg√≥**:

```bash
ollama list
```

**Salida esperada**:
```
NAME            ID          SIZE      MODIFIED
llama3.2:3b     a1b2c3d4    2.0 GB    2 minutes ago
```

---

### **PASO 6: Probar el Modelo**

1. **En la misma terminal**, ejecutar test:

```bash
ollama run llama3.2:3b "Analiza esta oferta de trabajo: Software Engineer for young and dynamic team"
```

2. **Esperar respuesta** (10-15 segundos primera vez):
   - El modelo analizar√° y retornar√° texto

3. **Si ves texto de an√°lisis** ‚Üí ‚úÖ **Funcionando correctamente**

4. **Salir del test**:
   - Presionar `Ctrl+D` o escribir `/bye`

5. **Salir del contenedor**:

```bash
exit
```

---

### **PASO 7: Configurar Variables de Entorno en Next.js**

1. **En tu PC**, abrir el archivo `.env.local` del proyecto

2. **A√±adir al FINAL del archivo**:

```bash
# ============================================================================
# OLLAMA - LLM LOCAL PARA AN√ÅLISIS DE INCLUSIVIDAD
# ============================================================================
# URL del servidor Ollama en VPS Hostinger
OLLAMA_HOST=http://77.83.232.203:11434
OLLAMA_MODEL=llama3.2:3b
```

3. **Guardar el archivo** (`Ctrl+S`)

4. **Reiniciar el servidor Next.js**:
   - Detener: `Ctrl+C` en la terminal donde corre `npm run dev`
   - Iniciar: `npm run dev`

---

## üîç Verificaci√≥n del Despliegue

### **Test desde tu app local**

Crear archivo temporal `test-ollama.js`:

```javascript
// test-ollama.js
const OLLAMA_HOST = 'http://77.83.232.203:11434'

async function testOllama() {
  try {
    const response = await fetch(`${OLLAMA_HOST}/api/generate`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: 'llama3.2:3b',
        prompt: 'Say hello in one sentence',
        stream: false
      })
    })
    
    const data = await response.json()
    console.log('‚úÖ Ollama responde:', data.response)
  } catch (error) {
    console.error('‚ùå Error:', error.message)
  }
}

testOllama()
```

**Ejecutar**:
```bash
node test-ollama.js
```

**Salida esperada**:
```
‚úÖ Ollama responde: Hello! How can I help you today?
```

---

## üìä Arquitectura Final

```
Internet
    ‚Üì
Next.js App (Local/Vercel)
    ‚Üì HTTP Request
VPS Hostinger (77.83.232.203:11434)
    ‚Üì
Dokploy
    ‚Üì
Docker Container (diversia-ollama)
    ‚Üì
Ollama Service
    ‚Üì
Llama 3.2 3B Model (~2 GB en RAM)
    ‚Üì
JSON Response ‚Üê An√°lisis
```

---

## üîí Seguridad

### **Puertos Expuestos**
- **11434**: Ollama API (p√∫blico, sin autenticaci√≥n)

‚ö†Ô∏è **Recomendaci√≥n**: En producci√≥n, configurar firewall para permitir solo:
- IP de Vercel (si despliegas ah√≠)
- Tu IP local (para desarrollo)

### **Datos Sensibles**
- ‚úÖ Ollama corre en tu VPS (no env√≠as datos a terceros)
- ‚úÖ 100% GDPR compliant (datos no salen de tu infraestructura)
- ‚úÖ No hay API keys externas

---

## üõ†Ô∏è Mantenimiento

### **Actualizar el modelo**

```bash
# Acceder al contenedor
docker exec -it diversia-ollama bash

# Actualizar modelo
ollama pull llama3.2:3b

# Salir
exit
```

### **Ver logs del contenedor**

En Dokploy:
1. Ir a proyecto "Diversia"
2. Hacer clic en servicio `diversia-ollama`
3. Pesta√±a "Logs"

### **Reiniciar servicio**

En Dokploy:
1. Servicio `diversia-ollama`
2. Bot√≥n "Restart"

---

## üìä Consumo de Recursos

### **RAM Utilizada**

| Servicio | RAM |
|----------|-----|
| Supabase (otro proyecto) | ~2.5 GB |
| Ollama + Llama 3.2 3B | ~2.5 GB |
| Sistema Ubuntu | ~0.5 GB |
| Docker overhead | ~0.3 GB |
| **Total** | **~5.3 GB de 8 GB** |

**Margen disponible**: ~2.7 GB ‚úÖ

### **Disco**

| Item | Espacio |
|------|---------|
| Imagen Ollama | ~800 MB |
| Modelo Llama 3.2 3B | ~2 GB |
| Volumen `ollama-data` | ~50 MB |
| **Total** | **~2.4 GB de 100 GB** |

---

## ‚ö†Ô∏è Limitaciones Conocidas

1. **CPU (2 cores)**:
   - Latencia de respuesta: **3-5 segundos** por an√°lisis
   - Solo 1 request simult√°nea recomendada

2. **RAM (8 GB)**:
   - **Modelo Phi-3 Mini (3.8B)**: No cabe con Supabase corriendo
   - **Llama 3.2 3B**: Funciona OK, ~2.5GB RAM

3. **Sin GPU**:
   - Respuestas m√°s lentas que con GPU (10x m√°s lento)
   - Suficiente para an√°lisis batch de jobs

---

## üöÄ Pr√≥ximos Pasos

1. ‚úÖ Infraestructura desplegada
2. ‚úÖ Modelo funcionando
3. ‚è≥ Integraci√≥n con Next.js (`app/lib/llm.js`)
4. ‚è≥ Implementar `analyzeJobInclusivity()`
5. ‚è≥ Tests y validaci√≥n

---

## üìû Troubleshooting

### Problema: "Connection refused" al puerto 11434

**Soluci√≥n**:
1. Verificar que contenedor est√© corriendo:
   ```bash
   docker ps | grep ollama
   ```
2. Verificar en Dokploy que servicio est√° "running" (verde)
3. Reiniciar contenedor si est√° stopped

### Problema: Modelo responde muy lento (>30s)

**Soluci√≥n**:
- Es normal en CPU sin GPU
- Si es cr√≠tico, migrar a Modal.com o RunPod con GPU

### Problema: "Out of memory"

**Soluci√≥n**:
1. Detener Supabase temporalmente:
   ```bash
   docker stop <supabase-container-name>
   ```
2. Usar modelo m√°s peque√±o (Gemma 2B o Gemma 3 1B como alternativa m√≠nima)

---

**Documentaci√≥n creada**: 24 de enero de 2026  
**√öltima actualizaci√≥n**: 24 de enero de 2026  
**Autor**: GACE + TECH_STACK_AGENT
**Modelo actualizado**: Gemma 2B ‚Üí Llama 3.2 3B (25 Feb 2026, decision GDPR Art. 9)
